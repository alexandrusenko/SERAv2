runtime:
  backend: "llama_cpp"  # llama_cpp | lmstudio
  model_path: "C:/Users/Xandr/Documents/models/qwen2.5-coder-32b-instruct-q3_k_m.gguf"
  n_ctx: 32768
  n_gpu_layers: -1
  n_batch: 1024
  threads: 12
  temperature: 0.15
  top_p: 0.92
  repeat_penalty: 1.05
  verbose: false
  lmstudio_base_url: "http://127.0.0.1:1234"
  lmstudio_model: "local-model"

memory:
  db_path: "data/memory.sqlite3"
  max_results: 12
  long_term_chunk_size: 8
  short_term_search_limit: 40
  long_term_search_limit: 40

safety:
  allow_shell: false
  allow_network: true
  allow_python_execution: false
  working_dir: "workspace"

log_level: "INFO"
planner_max_steps: 12
improve_after_failures: 2
